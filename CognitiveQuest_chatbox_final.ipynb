{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praise-Atadja/EIICD_chatbox/blob/main/CognitiveQuest_chatbox_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bGXGwrGhp7j"
      },
      "source": [
        "\n",
        "\n",
        "# **PROJECT NAME:**\n",
        "\n",
        "##EARLY INTERVENTION INTELLIGENCE FOR COGNITIVE DEVELOPMENT (EIICD) CHATBOX\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrfETgepDqPZ",
        "outputId": "3d1f0d93-fe8e-465c-bf89-ea2b0c51dcac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "id": "DqJUdBHRrpv1",
        "outputId": "e0de243f-4ea7-4f27-a1e7-11f096289ca0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.31.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_m1VKAIXdI5",
        "outputId": "9d4e3b57-7052-48f7-8a5b-3a99bbffe94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMQM7UhScrt6",
        "outputId": "75cd0708-f6f4-42f0-f358-6ee7f36cddd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gc2u7CfVCEcR",
        "outputId": "1248b65d-3575-4e55-f7e1-7f590d5f3f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, TrainingArguments, Trainer,  DataCollatorWithPadding, AutoModelForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics import classification_report\n",
        "import joblib\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A0yf9nAY_vgn"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "def stemSentence(sentence):\n",
        "    porter = PorterStemmer()\n",
        "    token_words=word_tokenize(sentence)\n",
        "    token_words\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay-utDPcxsne"
      },
      "source": [
        "## THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ro9sny39rlLj",
        "outputId": "231809cf-d006-40ea-8a94-a2768d563f0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Context', 'Questions', 'Answers', 'Patterns', 'Articles'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "dataset = load_dataset('csv', data_files='/content/drive/MyDrive/Autism Articles/Autsim_Q&A_datatset.csv')\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "87e9stcdKpiJ"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation sets\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fULFTxS_OIBL",
        "outputId": "f6ed3fc4-d22e-49a3-e3b2-df4c51d37393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 40\n",
            "Number of validation examples: 10\n"
          ]
        }
      ],
      "source": [
        "# Display the number of examples in the training and validation sets\n",
        "print(f\"Number of training examples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation examples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CNWUyaAtOUea",
        "outputId": "b750e621-db0f-4aa7-cc90-32b5871cdd56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Context': ['Parents can support by creating structured routines, using visual aids, and providing consistent, positive reinforcement.', 'ABA therapy uses behavior principles to improve social, communication, and learning skills.', 'Siblings can be supported through education about autism, encouragement of open communication, and ensuring they have their own outlets for support and attention.', 'Genetics play a significant role in autism, as evidenced by higher concordance rates among identical twins and the identification of specific genetic mutations associated with autism spectrum disorder. However, not all cases of autism can be explained by genetic factors alone, suggesting a complex interplay of genetic and environmental influences.', \"Autism severity levels (mild, moderate, severe) are based on the amount of support an individual requires across different domains (social communication, repetitive behaviors, sensory issues). These levels guide intervention and support strategies tailored to the individual's specific needs and functional abilities.\"], 'Questions': ['How can parents support a child with autism at home?', 'What is Applied Behavior Analysis (ABA) therapy?', 'How can siblings of children with autism be supported?', 'What role does genetics play in autism?', 'What are the different levels of autism severity?'], 'Answers': [\"Parents can support their child with autism by establishing routines, using visual supports, promoting communication, providing sensory-friendly environments, and participating in therapies and educational activities tailored to their child's needs.\", 'ABA therapy is a structured, evidence-based intervention that focuses on improving socially significant behaviors, reducing challenging behaviors, and teaching new skills through positive reinforcement and systematic teaching methods.', 'Siblings can be supported through education about autism, promoting understanding and empathy, providing opportunities for bonding and shared activities, and offering emotional support and resources for coping with the unique challenges and rewards of having a sibling with autism.', 'Genetics plays a significant role in autism, with evidence suggesting that certain genetic mutations and variations contribute to autism susceptibility. However, autism is complex, and genetic factors interact with environmental influences.', 'Autism severity is often categorized into three levels based on the level of support needed: Level 1 (requiring support), Level 2 (requiring substantial support), and Level 3 (requiring very substantial support). These levels reflect the degree of impairment in social communication, behavior, and adaptive functioning.'], 'Patterns': ['How do parents help autistic children at home?', 'What is ABA therapy?', 'How do you support siblings of autistic kids?', 'What is the genetic link to autism?', 'What are autism severity levels?'], 'Articles': ['https://www.autismspeaks.org/family-support-tool-kit', 'https://www.autismspeaks.org/applied-behavior-analysis-aba-0', 'https://www.autismspeaks.org/family-dynamics-and-siblings-autism', 'https://www.autismspeaks.org/what-autism/learn-more-autism/what-causes-autism', 'https://www.autism.org.uk/about/what-is/asperger']}\n",
            "{'Context': ['Common symptoms of autism include difficulties in social interaction (such as understanding social cues), challenges in communication (verbal and non-verbal), repetitive behaviors (like hand-flapping), sensory sensitivities (to light, sound, touch), and sometimes, intellectual challenges or specific interests.', 'Myths about autism include misconceptions about its causes (like vaccines), stereotypes about behaviors (such as all individuals with autism being non-verbal or having savant abilities), and misunderstandings about treatment effectiveness. Educating the public about autism helps dispel myths and promotes acceptance and support for individuals with autism.', 'Common co-occurring conditions with autism include intellectual disabilities, epilepsy, ADHD (Attention Deficit Hyperactivity Disorder), anxiety disorders, gastrointestinal issues, and sensory processing disorders. Addressing these conditions often requires a multidisciplinary approach to treatment and support.', 'Technology can assist by providing communication aids (like speech-generating devices), social skills apps, sensory tools (noise-canceling headphones), and virtual support networks.', 'Strategies include using visual schedules, providing warnings before transitions, and offering transition supports like social stories or countdown timers.'], 'Questions': ['What are some common symptoms of autism?', 'What are some myths about autism that need to be debunked?', 'What are some common co-occurring conditions with autism?', 'How can technology assist individuals with autism?', 'What are some strategies for managing transitions for individuals with autism?'], 'Answers': ['Common symptoms include challenges in social communication and interaction, restricted interests, repetitive behaviors, sensory sensitivities, and sometimes intellectual disabilities or delays in language development.', 'Myths about autism include misconceptions about its causes (e.g., vaccines), stereotypes about behaviors and abilities, and outdated beliefs about autism being solely a childhood disorder. Understanding and dispelling myths promote accurate awareness and acceptance of autism.', 'Common co-occurring conditions with autism include ADHD (Attention-Deficit/Hyperactivity Disorder), anxiety disorders, epilepsy, gastrointestinal issues, sensory processing disorders, and sleep disturbances.', 'Technology can assist individuals with autism by providing communication aids (e.g., AAC devices), social skills apps, sensory regulation tools, virtual supports for remote learning, and assistive technologies that support independence and participation in daily activities.', 'Strategies include using visual schedules, providing advance notice of changes, offering transition objects or routines, using positive reinforcement, and creating a predictable environment to reduce anxiety and support smooth transitions for individuals with autism.'], 'Patterns': ['What are typical autism symptoms?', 'What myths about autism are false?', 'What other conditions come with autism?', 'How does tech help autism?', 'How do you manage transitions for autism?'], 'Articles': ['https://www.mayoclinic.org/diseases-conditions/autism-spectrum-disorder/symptoms-causes/syc-20352928', 'https://www.autism-society.org/myths-facts/', 'https://www.cdc.gov/ncbddd/autism/facts.html', 'https://www.autismspeaks.org/assistive-technology-autism', 'https://www.autism.org.uk/advice-and-guidance']}\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[:5])\n",
        "print(val_dataset[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DlqyKO-rOkU0",
        "outputId": "027bf4cf-e6eb-4c3f-d66b-35eade800405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Context': 'Parents can support by creating structured routines, using visual aids, and providing consistent, positive reinforcement.',\n",
              " 'Questions': 'How can parents support a child with autism at home?',\n",
              " 'Answers': \"Parents can support their child with autism by establishing routines, using visual supports, promoting communication, providing sensory-friendly environments, and participating in therapies and educational activities tailored to their child's needs.\",\n",
              " 'Patterns': 'How do parents help autistic children at home?',\n",
              " 'Articles': 'https://www.autismspeaks.org/family-support-tool-kit'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0Na-hLGTSQm8",
        "outputId": "bf5b0bfb-d86c-4079-efc2-473bcf09489f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset saved to train_dataset.json\n",
            "Validation dataset saved to val_dataset.json\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Convert Dataset objects to lists of dictionaries\n",
        "dataset_dict = [example for example in train_dataset]\n",
        "train_data_dict = [example for example in train_dataset]\n",
        "val_data_dict = [example for example in val_dataset]\n",
        "\n",
        "# Define file paths\n",
        "train_file = 'train_dataset.json'\n",
        "val_file = 'val_dataset.json'\n",
        "dataset_file = 'dataset.json'\n",
        "\n",
        "# Save training dataset to JSON\n",
        "with open(train_file, 'w') as f:\n",
        "    json.dump(train_data_dict, f, indent=4)\n",
        "\n",
        "# Save validation dataset to JSON\n",
        "with open(val_file, 'w') as f:\n",
        "    json.dump(val_data_dict, f, indent=4)\n",
        "\n",
        "# Save dataset to JSON\n",
        "with open(dataset_file, 'w') as f:\n",
        "    json.dump(dataset_dict, f, indent=4)\n",
        "\n",
        "print(f\"Training dataset saved to {train_file}\")\n",
        "print(f\"Validation dataset saved to {val_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset)"
      ],
      "metadata": {
        "id": "ivi_fpsjX6Sd",
        "outputId": "c52d266a-9754-4ece-b602-601550e386d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Context', 'Questions', 'Answers', 'Patterns', 'Articles'],\n",
            "    num_rows: 10\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hSoqJNqzD8zw"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters and digits\n",
        "    text = ' '.join(text.split())  # Remove extra whitespaces\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q42DjuSvTPU_",
        "outputId": "c898202d-9507-47ff-ccf6-f3ab5708132f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of input_ids: 40\n",
            "Number of attention_masks: 40\n",
            "Number of labels: 40\n",
            "Number of skipped samples: 0\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load the tokenizer\n",
        "# Define the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def get_unique_answers(dataset):\n",
        "    \"\"\"Extracts unique answers from the dataset.\"\"\"\n",
        "    unique_answers = set()\n",
        "    for sample in dataset:\n",
        "        answer = sample.get('Answers')\n",
        "        if answer is not None:\n",
        "            unique_answers.add(answer)\n",
        "    return unique_answers\n",
        "\n",
        "# Get unique answers from your dataset\n",
        "unique_answers = get_unique_answers(train_dataset)  # Assuming train_dataset has all possible answers\n",
        "\n",
        "# Create label mapping dynamically\n",
        "label_to_id = {answer: idx for idx, answer in enumerate(unique_answers)}\n",
        "\n",
        "def tokenize_dataset(dataset, tokenizer=tokenizer, label_to_id=label_to_id):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    labels_list = []\n",
        "    skipped_samples = 0  # Initialize a counter for skipped samples\n",
        "\n",
        "    for sample in dataset:\n",
        "        answer = sample.get('Answers')\n",
        "        if answer is None:\n",
        "            # Skip samples with missing 'Answers'\n",
        "            print(f\"Warning: 'Answers' key not found or has None value in sample: {sample}\")\n",
        "            skipped_samples += 1  # Increment the counter\n",
        "            continue\n",
        "\n",
        "        label = label_to_id.get(answer)\n",
        "        if label is None:\n",
        "            print(f\"Warning: Label not found for answer '{answer}' in sample: {sample}\")\n",
        "            skipped_samples += 1  # Increment the counter\n",
        "            continue\n",
        "\n",
        "        tokenized_example = tokenizer(\n",
        "            sample.get(\"Questions\", \"\"),\n",
        "            sample.get(\"Context\", \"\"),  # Assuming you changed 'Patterns' to 'Context'\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids.append(tokenized_example['input_ids'].squeeze())\n",
        "        attention_masks.append(tokenized_example['attention_mask'].squeeze())\n",
        "        labels_list.append(label)\n",
        "\n",
        "    if not input_ids:\n",
        "        raise RuntimeError(\"No valid samples found. Check your dataset and label mapping.\")\n",
        "\n",
        "    input_ids = torch.stack(input_ids)\n",
        "    attention_masks = torch.stack(attention_masks)\n",
        "    labels_tensor = torch.tensor(labels_list, dtype=torch.long)\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_masks, 'labels': labels_tensor, 'skipped_samples': skipped_samples}\n",
        "\n",
        "# Tokenize the datasets\n",
        "tokenized_train_datasets = tokenize_dataset(train_dataset)\n",
        "\n",
        "\n",
        "print(f\"Number of input_ids: {tokenized_train_datasets['input_ids'].size(0)}\")\n",
        "print(f\"Number of attention_masks: {tokenized_train_datasets['attention_mask'].size(0)}\")\n",
        "print(f\"Number of labels: {tokenized_train_datasets['labels'].size(0)}\")\n",
        "print(f\"Number of skipped samples: {tokenized_train_datasets['skipped_samples']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenize datasets\n",
        "save_path = 'tokenized_train_datasets'\n",
        "torch.save(tokenized_train_datasets, save_path)\n"
      ],
      "metadata": {
        "id": "dXUqUIL3jwZU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qx4YNKTqzQTT",
        "outputId": "9e72c552-fa55-44bd-c811-ca331ac371c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of final_train_dataset: 40\n"
          ]
        }
      ],
      "source": [
        "# Define MyDataset class\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, tokenized_dataset):\n",
        "        self.input_ids = tokenized_dataset['input_ids']\n",
        "        self.attention_mask = tokenized_dataset['attention_mask']\n",
        "        self.labels = tokenized_dataset['labels']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            'input_ids': self.input_ids[idx],\n",
        "            'attention_mask': self.attention_mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Prepare the datasets\n",
        "# Load your own tokenized dataset (example using pickle)\n",
        "with open('tokenized_train_datasets', 'rb') as f:  # Use the correct filename\n",
        "    tokenized_train_dataset = torch.load(f)  # Load with torch.load\n",
        "\n",
        "final_train_dataset = MyDataset(tokenized_train_dataset)\n",
        "\n",
        "# After creating the datasets, print their lengths\n",
        "print(\"Length of final_train_dataset:\", len(final_train_dataset))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG57PtY8yd0j"
      },
      "source": [
        "EVALUATING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "with open('final_train_dataset', 'wb') as f:\n",
        "    pickle.dump(final_train_dataset, f)\n",
        "\n",
        "with open('final_train_dataset', 'rb') as f:\n",
        "    final_train_dataset = pickle.load(f)\n",
        "\n",
        "# Define model and tokenizer\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Use tokenized_train_dataset instead of tokenized_datasets\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(tokenized_train_dataset['labels']))\n",
        "\n",
        "# Define Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./output', # Add the output directory\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    evaluation_strategy=\"no\",\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=final_train_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('final_model')\n",
        "tokenizer.save_pretrained('final_model')"
      ],
      "metadata": {
        "id": "jveTOxFm_Krx",
        "outputId": "f54e811b-077b-40a7-8efa-91eadae590b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_runtime': 790.2442, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.038, 'train_loss': 3.743750254313151, 'epoch': 3.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('final_model/tokenizer_config.json',\n",
              " 'final_model/special_tokens_map.json',\n",
              " 'final_model/vocab.txt',\n",
              " 'final_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/content/drive/MyDrive/EIICD_Chatbot/final_model\"\n",
        "tokenizer_path = \"/content/drive/MyDrive/EIICD_Chatbot/final_model_tokenizer\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(tokenizer_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")\n",
        "print(f\"Tokenizer saved to: {tokenizer_path}\")\n"
      ],
      "metadata": {
        "id": "KblUNRZ_JyJC",
        "outputId": "4f6af53b-4a10-4f66-e06b-a859a682bfff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Model saved to: /content/drive/MyDrive/EIICD_Chatbot/final_model\n",
            "Tokenizer saved to: /content/drive/MyDrive/EIICD_Chatbot/final_model_tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(predictions, labels):\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    cm = confusion_matrix(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm\n",
        "    }\n"
      ],
      "metadata": {
        "id": "QKUjFZaycI8M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, dataset):\n",
        "    model.eval()\n",
        "\n",
        "    input_ids = dataset['input_ids']\n",
        "    attention_mask = dataset['attention_mask']\n",
        "    labels = dataset['labels']\n",
        "\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for i in range(len(input_ids)):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids[i].unsqueeze(0), attention_mask=attention_mask[i].unsqueeze(0))\n",
        "            logits = outputs.logits\n",
        "            predictions.append(logits.numpy())\n",
        "            true_labels.append(labels[i].numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_labels = np.array(true_labels)\n",
        "\n",
        "    metrics = compute_metrics(predictions, true_labels)\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "an4hLi84cNnh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "metrics = evaluate_model(model, tokenizer, tokenized_train_datasets)\n",
        "\n",
        "# Print out the metrics\n",
        "print(f\"Accuracy: {metrics['accuracy']}\")\n",
        "print(f\"Precision: {metrics['precision']}\")\n",
        "print(f\"Recall: {metrics['recall']}\")\n",
        "print(f\"F1 Score: {metrics['f1']}\")\n",
        "print(f\"Confusion Matrix: \\n{metrics['confusion_matrix']}\")"
      ],
      "metadata": {
        "id": "g9YT3u88cWDa",
        "outputId": "68857669-a8d4-455a-dddd-254372bce49f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.075\n",
            "Precision: 0.011388888888888888\n",
            "Recall: 0.075\n",
            "F1 Score: 0.018256578947368422\n",
            "Confusion Matrix: \n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained('final_model')  # Assuming './saved_model' is where your trained model is saved\n",
        "\n",
        "# Function to get answer from model\n",
        "def get_answer(input_text):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(input_text, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Convert logits to probabilities using softmax\n",
        "    probs = torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    # Get predicted label (answer)\n",
        "    predicted_label_id = torch.argmax(probs, dim=-1).item()\n",
        "\n",
        "    # Reverse label_to_id mapping to get answer string\n",
        "    id_to_label = {v: k for k, v in label_to_id.items()}\n",
        "    predicted_answer = id_to_label[predicted_label_id]\n",
        "\n",
        "    return predicted_answer\n",
        "\n",
        "# Example usage:\n",
        "input_text = \"What is autism spectrum disorder?\"\n",
        "predicted_answer = get_answer(input_text)\n",
        "print(f\"Predicted answer: {predicted_answer}\")\n"
      ],
      "metadata": {
        "id": "_8Tf3tCouPc5",
        "outputId": "3949a04c-00f0-429b-ef70-283c3f43fc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted answer: Autism may impact family dynamics by requiring adjustments in parenting approaches, time management, financial resources, and social activities. Family members may experience stress but also opportunities for growth, understanding, and advocacy for their loved one with autism.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "input_text = \"What is autism spectrum disorder?\"\n",
        "predicted_answer = get_answer(input_text)\n",
        "print(f\"Predicted answer: {predicted_answer}\")"
      ],
      "metadata": {
        "id": "FC56nRaAutHU",
        "outputId": "d0fffe13-c20c-41d5-dd97-1d656902898c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted answer: Autism may impact family dynamics by requiring adjustments in parenting approaches, time management, financial resources, and social activities. Family members may experience stress but also opportunities for growth, understanding, and advocacy for their loved one with autism.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzBSEgdkv/d4+VpYpk6tnR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}