{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ3BTvTQEJEK88X2N71vAG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praise-Atadja/EIICD_chatbox/blob/main/EIICD_chatbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# **PROJECT NAME:**\n",
        "\n",
        "##EARLY INTERVENTION INTELLIGENCE FOR COGNITIVE DEVELOPMENT (EIICD) CHATBOX\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5bGXGwrGhp7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc2u7CfVCEcR",
        "outputId": "992867bc-930c-4590-f6cb-b0d6faa0d32d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import re\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/AutsimChatbox_datatset.csv')\n",
        "\n",
        "# Function to clean and preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "3P1QUjpJfoTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained BERT model\n",
        "model = TFBertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = ' '.join(text.split())  # Remove extra whitespaces\n",
        "    return text"
      ],
      "metadata": {
        "id": "BHR66uaafvp2",
        "outputId": "fff07914-fa85-48e8-a7d3-2df9d62ac163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare input tensors for BERT\n",
        "def prepare_input_tensors(question, reference):\n",
        "    quest_toks = tokenizer.tokenize(question)\n",
        "    text_toks = tokenizer.tokenize(reference)\n",
        "    tokens = ['[CLS]'] + quest_toks + ['[SEP]'] + text_toks + ['[SEP]']\n",
        "\n",
        "    input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    mask = [1] * len(input_id)\n",
        "    input_type = [0] * (1 + len(quest_toks) + 1) + [1] * (len(text_toks) + 1)\n",
        "\n",
        "    input_id, mask, input_type = map(lambda x: tf.convert_to_tensor(x, dtype=tf.int32), (input_id, mask, input_type))\n",
        "\n",
        "    return input_id, mask, input_type"
      ],
      "metadata": {
        "id": "aW1SFzUAf2vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a dataset from CSV\n",
        "def create_dataset_from_csv(csv_file, batch_size=16):\n",
        "    data = pd.read_csv(csv_file)\n",
        "    Questions = data['Questions'].apply(preprocess_text).tolist()\n",
        "    Answers = data['Answers'].apply(preprocess_text).tolist()\n",
        "    Patterns = data['Patterns'].apply(preprocess_text).tolist()\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, start_positions, end_positions = [], [], [], [], []\n",
        "\n",
        "    for Question, Answer in zip(Questions, Answers):\n",
        "        input_id, attention_mask, token_type_id = prepare_input_tensors(Question, Answer)\n",
        "\n",
        "        answer_tokens = tokenizer.tokenize(Answer)\n",
        "        answer_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n",
        "\n",
        "        start_position = tf.where(tf.equal(input_id, answer_ids[0]))[0][0].numpy()\n",
        "        end_position = start_position + len(answer_ids) - 1\n",
        "\n",
        "        input_ids.append(tf.expand_dims(input_id, 0))\n",
        "        attention_masks.append(tf.expand_dims(attention_mask, 0))\n",
        "        token_type_ids.append(tf.expand_dims(token_type_id, 0))\n",
        "        start_positions.append(start_position)\n",
        "        end_positions.append(end_position)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "        'input_ids': tf.concat(input_ids, axis=0),\n",
        "        'attention_mask': tf.concat(attention_masks, axis=0),\n",
        "        'token_type_ids': tf.concat(token_type_ids, axis=0)\n",
        "    }, {\n",
        "        'start_positions': tf.convert_to_tensor(start_positions, dtype=tf.int32),\n",
        "        'end_positions': tf.convert_to_tensor(end_positions, dtype=tf.int32)\n",
        "    }))\n",
        "\n",
        "    return dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "kI5asp-CgCEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_csv(csv_file, batch_size=16, max_length=128): # Added max_length\n",
        "    data = pd.read_csv(csv_file)\n",
        "    Questions = data['Questions'].apply(preprocess_text).tolist()\n",
        "    Answers = data['Answers'].apply(preprocess_text).tolist()\n",
        "    Patterns = data['Patterns'].apply(preprocess_text).tolist()\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids, start_positions, end_positions = [], [], [], [], []\n",
        "\n",
        "    for Question, Answer in zip(Questions, Answers):\n",
        "        # Tokenize and convert to IDs, limiting sequence length\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            Question,\n",
        "            Answer,\n",
        "            max_length=max_length,  # Truncate or pad sequences\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "\n",
        "        input_id = encoded['input_ids'][0]\n",
        "        attention_mask = encoded['attention_mask'][0]\n",
        "        token_type_id = encoded['token_type_ids'][0]\n",
        "\n",
        "        answer_tokens = tokenizer.tokenize(Answer)\n",
        "        answer_ids = tokenizer.convert_tokens_to_ids(answer_tokens)\n",
        "\n",
        "        input_id_np = input_id.numpy().tolist()\n",
        "\n",
        "        # Handle cases where answer is not found\n",
        "        if answer_ids[0] in input_id_np:\n",
        "            start_position = input_id_np.index(answer_ids[0])\n",
        "            end_position = start_position + len(answer_ids) - 1\n",
        "        else:\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "        input_ids.append(tf.expand_dims(input_id, 0))\n",
        "        attention_masks.append(tf.expand_dims(attention_mask, 0))\n",
        "        token_type_ids.append(tf.expand_dims(token_type_id, 0))\n",
        "        start_positions.append(start_position)\n",
        "        end_positions.append(end_position)\n",
        "\n",
        "    # All tensors now have the same shape due to padding/truncation\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "        'input_ids': tf.concat(input_ids, axis=0),\n",
        "        'attention_mask': tf.concat(attention_masks, axis=0),\n",
        "        'token_type_ids': tf.concat(token_type_ids, axis=0)\n",
        "    }, {\n",
        "        'start_positions': tf.convert_to_tensor(start_positions, dtype=tf.int32),\n",
        "        'end_positions': tf.convert_to_tensor(end_positions, dtype=tf.int32)\n",
        "    }))\n",
        "\n",
        "    return dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "GR7fzhlsuExV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model\n",
        "train_dataset = create_dataset_from_csv('/content/drive/MyDrive/AutsimChatbox_datatset.csv')\n",
        "\n",
        "# Define the loss function\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Pass the optimizer as a string identifier\n",
        "model.compile(optimizer='adam', loss=loss)  # Use 'adam' instead of optimizer object\n",
        "model.fit(train_dataset, epochs=3)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save('fine_tuned_bert_model')"
      ],
      "metadata": {
        "id": "o1PEtn-O2weI",
        "outputId": "b88affe3-85ab-44b3-de62-3a76edeb938a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x77fd34d89000> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x77fd34d89000> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Semantic search function using Universal Sentence Encoder\n",
        "# Load Universal Sentence Encoder (USE) model\n",
        "use_model = hub.load('https://tfhub.dev/google/universal-sentence-encoder-large/5')\n",
        "\n",
        "# Function for semantic search using Universal Sentence Encoder (USE)\n",
        "def semantic_search(corpus, query):\n",
        "    query_embedding = use_model([query])[0]\n",
        "    corpus_embeddings = use_model(corpus)\n",
        "    similarity = np.inner(query_embedding, corpus_embeddings)\n",
        "    closest = np.argmax(similarity)\n",
        "    return corpus[closest]\n",
        "\n",
        "# Example usage of semantic search\n",
        "data = pd.read_csv('/content/drive/MyDrive/AutsimChatbox_datatset.csv')\n",
        "corpus = data['Patterns'].apply(preprocess_text).tolist()\n",
        "query = \"How does autism affect sleep patterns?\"\n",
        "\n",
        "most_similar_text = semantic_search(corpus, query)\n",
        "print(\"Most similar text:\", most_similar_text)"
      ],
      "metadata": {
        "id": "epKB-KBf23yD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}